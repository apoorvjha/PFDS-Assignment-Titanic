{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survivor Liklihood Prediction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this Endevour, a culmination of efforts in the <b>PFDS</b> course. In this assignment, we delve into the intriguing world of <b>predictive modeling</b>, employing the renowned <b><u>Titanic dataset</u></b>. Our goal is to construct a <b>robust predictive model</b> capable of determining the <b><u><i>likelihood of survival</i></u></b> for passengers aboard the ill-fated Titanic.\n",
    "\n",
    "The <b><u>Titanic dataset</u></b> is a classic in the realm of data science, offering a rich tapestry of information about passengers, their socio-economic status, and survival outcomes. Leveraging this dataset, we embark on a journey through the essential steps of <b>data exploration</b>, <b>preprocessing</b>, <b>feature engineering</b>, and <b>model development</b>. Along the way, we'll employ various machine learning techniques to train and evaluate our predictive model.\n",
    "\n",
    "This notebook serves as a comprehensive documentation of our analytical process, providing insights into the decisions made, challenges encountered, and the methodologies employed. Through meticulous coding and detailed explanations, we aim to not only showcase our technical proficiency but also our understanding of the underlying principles of predictive modeling.\n",
    "\n",
    "Throughout this assignment, we adhere to the best practices of data science, emphasizing clarity, reproducibility, and thoughtful analysis. As we traverse each section, we encourage you to explore the code, interpret the results, and gain a deeper understanding of the intricacies involved in building a predictive model.\n",
    "\n",
    "Let's set sail into the world of data science and uncover the patterns that may illuminate the factors contributing to survival on the Titanic. May this notebook be a testament to our dedication to mastering the foundations of data science and our ability to apply these skills to real-world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "import mlflow\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "This is library of custom functions developed to help in data cleaning, preprocessing and making data model ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, logger, sheet_name = 0, usecols = None, concat_sheets = False):\n",
    "    \"\"\"\n",
    "    Description : This function reads the file present in the specified path and returns the dataframe in case of csv or excel files \n",
    "    and dictionary in case of json files.\n",
    "\n",
    "    Inputs:\n",
    "        path [STRING] : The path to the file to be read.\n",
    "        logger [LOGGING.LOGGER] : The logger instance to keep track of errors and other information for debugging.\n",
    "        sheet_name [INTEGER / STRING] : The qualified index or name of sheet in case of excel files.\n",
    "        usecols [STRING] : The column indexes to be subsetted in entire excel file.\n",
    "        concat_sheets [BOOLEAN] : If the sheets present in excel file needs to be concatenated into single dataframe.\n",
    "\n",
    "    Outputs:\n",
    "        data [DATAFRAME / DICTIONARY] : The contents of file in dataframe (in case of CSV or Excel) or in Dictionary (in case of JSON) \n",
    "    \"\"\"\n",
    "    file_extension = path.split('.')[-1]\n",
    "    logger.info(f\"The file extension is {file_extension}!\")\n",
    "    if file_extension == 'csv':\n",
    "        try:\n",
    "            data = pd.read_csv(path)\n",
    "            logger.info(f\"The file at path {path} read successfully with {data.shape[0]} rows and {data.shape[1]} columns!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"The read of file at path {path} failed due to {e}!\")\n",
    "    elif file_extension == 'xlsx':\n",
    "        try:\n",
    "            data = pd.read_excel(path, sheet_name = sheet_name, usecols = usecols)\n",
    "            logger.info(f\"The file at path {path} read successfully with {data.shape[0]} rows and {data.shape[1]} columns!\")\n",
    "            if concat_sheets == True:\n",
    "                concated_data = pd.DataFrame()\n",
    "                for key in data:\n",
    "                    concated_data = pd.concat([concated_data, data[key]] , axis = 0, ignore_index = True)\n",
    "            data = concated_data.copy()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"The read of file at path {path} failed due to {e}!\")\n",
    "    elif file_extension == 'json':\n",
    "        try:\n",
    "            with open(path, \"r\") as fd:\n",
    "                data = json.load(fd)\n",
    "            logger.info(f\"The file at path {path} read successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"The read of file at path {path} failed due to {e}!\")\n",
    "    else:\n",
    "        logger.error(f\"The extension {file_extension} is not supported!\")\n",
    "        data = pd.DataFrame()\n",
    "    return data\n",
    "\n",
    "def data_type_conversion(data, logger, mapping):\n",
    "    \"\"\"\n",
    "    Description : The function is reponsible to cast the columns into specific data types as specified in mapping file.\n",
    "\n",
    "    Inputs:\n",
    "        data [DATAFRAME] : The data in which data type conversion will be applied.\n",
    "        logger [LOGGING.LOGGER] : The logger instance to keep track of errors and other information for debugging.\n",
    "        mapping [DICTIONARY] : The mapping having keys as column names and values as target data type of the columns.\n",
    "\n",
    "    Outputs:\n",
    "        data [DATAFRAME] : TRansformed data.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Explicitly converting the data type of columns leveraging {mapping}\")\n",
    "    for column in mapping.keys():\n",
    "        try:\n",
    "            data[column] = data[column].astype(mapping[column])\n",
    "            logger.info(f\"Data type conversion for {column} into {mapping[column]} completed successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data type conversion for {column} into {mapping[column]} Failed due to {e}!\")\n",
    "    return data\n",
    "\n",
    "def string_encoding(data, logger, string_columns):\n",
    "    \"\"\"\n",
    "    Description : The function is reponsible to converting the columns having string data into one-hot vector encoded format.\n",
    "\n",
    "    Inputs:\n",
    "        data [DATAFRAME] : The data in which string encoding will be applied.\n",
    "        logger [LOGGING.LOGGER] : The logger instance to keep track of errors and other information for debugging.\n",
    "        string_columns [LIST] : The list of columns of string type for which one-hot vector encoding needs to be applied.\n",
    "\n",
    "    Outputs:\n",
    "        data [DATAFRAME] : TRansformed data.\n",
    "    \"\"\"\n",
    "    logger.info(f\"converting the string columns into neumeric features!\")\n",
    "    try:\n",
    "        data = pd.get_dummies(data, columns = string_columns)\n",
    "        logger.info(f\"convertion of string columns {string_columns} into neumeric features completed successfully!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"convertion of string columns {string_columns} into neumeric features failed due to {e}!\")\n",
    "    return data\n",
    "\n",
    "def missing_data_imputation(data, logger, column, method = \"median\"):\n",
    "    \"\"\"\n",
    "    Description : The function is reponsible to impute the NaNs present in the column of the data with appropriate values\n",
    "    based on the choice of method.\n",
    "\n",
    "    Inputs:\n",
    "        data [DATAFRAME] : The data in which missing data imputation will be applied.\n",
    "        logger [LOGGING.LOGGER] : The logger instance to keep track of errors and other information for debugging.\n",
    "        column [STRING] : The column in which missing data imputation will be applied.\n",
    "        method [STRING] : Specifies the method through which the NaNs will be replaced.\n",
    "                        - for method = 'mean', NaNs are replaced with mean of non-NaN records of the column.\n",
    "                        - for method = 'median', NaNs are replaced with median of non-NaN records of the column.\n",
    "                        - for method = 'mode', NaNs are replaced with mode of non-NaN records of the column.\n",
    "\n",
    "    Outputs:\n",
    "        data [DATAFRAME] : TRansformed data.\n",
    "    \"\"\"\n",
    "    assert method in ['median', 'mean', 'mode'], \"Only 'median', 'mean' and 'mode' inputation methods are supported!\"\n",
    "    logger.info(f\"Imputing missing values for {column} with {method}!\")\n",
    "    if method == 'mean':\n",
    "        try:\n",
    "            data[column].fillna(data[column].mean(), inplace = True)\n",
    "            logger.info(f\"Missing value imputation for {column} completed successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Missing value imputation for {column} failed due to {e}!\")\n",
    "    elif method == 'median':\n",
    "        try:\n",
    "            data[column].fillna(data[column].median(skipna = True), inplace = True)\n",
    "            logger.info(f\"Missing value imputation for {column} completed successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Missing value imputation for {column} failed due to {e}!\")\n",
    "    else:\n",
    "        try:\n",
    "            data[column].fillna(data[column].mode(dropna = True)[0], inplace = True)\n",
    "            logger.info(f\"Missing value imputation for {column} completed successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Missing value imputation for {column} failed due to {e}!\")\n",
    "    return data\n",
    "\n",
    "def create_quantile_feature(data, logger, column, q = 10):\n",
    "    \"\"\"\n",
    "    Description : The function is reponsible to create the quantile based trend features.\n",
    "\n",
    "    Inputs:\n",
    "        data [DATAFRAME] : The data in which new feature will be added.\n",
    "        logger [LOGGING.LOGGER] : The logger instance to keep track of errors and other information for debugging.\n",
    "        column [STRING] : The column which will be transformed to create the new feature.\n",
    "        q [INTEGER] : The quantile buckets that needs to be made in the data.\n",
    "\n",
    "    Outputs:\n",
    "        data [DATAFRAME] : TRansformed data.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating quantile feature with q = {q} for {column} column!\")\n",
    "    try:\n",
    "        data[column + f\"_{q}_quantile_feature\"] = pd.qcut(data[column], q = q, labels = False, retbins = False).tolist()\n",
    "        logger.info(f\"Created quantile feature with q = {q} for {column} column successfully!\")\n",
    "    except Exception as e:\n",
    "        logger.info(f\"quantile feature with q = {q} for {column} column creation failed due to {e}!\")\n",
    "    return data\n",
    "\n",
    "def split_train_validation(data, logger, features, target, test_size = 0.2, random_state = 45, shuffle = True):\n",
    "    \"\"\"\n",
    "    Description : The function is reponsible to split the input dataframe into training and validation sets.\n",
    "\n",
    "    Inputs:\n",
    "        data [DATAFRAME] : The data which will be splitted into two sets.\n",
    "        logger [LOGGING.LOGGER] : The logger instance to keep track of errors and other information for debugging.\n",
    "        features [LIST] : The list of column names to be used as input feature to the model.\n",
    "        target [STRING] : The target column which will be output of the predictive model. \n",
    "\n",
    "    Outputs:\n",
    "        data [DATAFRAME] : TRansformed data.\n",
    "    \"\"\"\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(data[features] , data[target], test_size = test_size, random_state = random_state,shuffle=shuffle)\n",
    "    return pd.DataFrame(data = X_train, columns = features), pd.DataFrame(data = X_val, columns = features), Y_train, Y_val\n",
    "\n",
    "def normalize(data, logger, scaler_path, fit_standard_scaler = True):\n",
    "    \"\"\"\n",
    "    Description : The function is reponsible to scale the data such that the mean is 0 and standard deviation is 1.\n",
    "\n",
    "    Inputs:\n",
    "        data [DATAFRAME] : The data in which scaling will be applied.\n",
    "        logger [LOGGING.LOGGER] : The logger instance to keep track of errors and other information for debugging.\n",
    "        scaler_path [STRING] : The path at which the standard scaler object will be saved or picked from.\n",
    "        fit_standard_scaler [BOOLEAN] : The flag that tells the function wheather to fit and transform a new instance of\n",
    "            standard scaler or use the pretrained one.\n",
    "\n",
    "    Outputs:\n",
    "        data [DATAFRAME] : TRansformed data.\n",
    "    \"\"\"\n",
    "    if fit_standard_scaler == True:\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(data = data, columns = scaler.feature_names_in_)\n",
    "        with open(scaler_path, \"wb\") as fd:\n",
    "            pickle.dump(scaler, fd)\n",
    "    else:\n",
    "        with open(scaler_path, \"rb\") as fd:\n",
    "            scaler = pickle.load(fd)\n",
    "        data = scaler.transform(data)\n",
    "        data = pd.DataFrame(data = data, columns = scaler.feature_names_in_)\n",
    "    return data\n",
    "\n",
    "def log_metrics(actual, predicted_labels, predicted_scores, model, model_name, random_state, val_ratio):\n",
    "    \"\"\"\n",
    "    TBU\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "    mlflow.set_experiment(\"PFDS Assignment\")\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_param(\"Model Name\", model_name)\n",
    "        mlflow.log_param(\"Random State\", random_state)\n",
    "        mlflow.log_param(\"validation_split_ratio\", val_ratio)\n",
    "        mlflow.log_artifact(\"../configuration/pipeline_configuration.json\")\n",
    "        mlflow.log_metric(\"Accuracy\", accuracy_score(actual, predicted_labels))\n",
    "        mlflow.log_metric(\"Precision\", precision_score(actual, predicted_labels))\n",
    "        mlflow.log_metric(\"Recall\", recall_score(actual, predicted_labels))\n",
    "        mlflow.log_metric(\"F1 Score\", f1_score(actual, predicted_labels))\n",
    "        mlflow.log_metric(\"ROC AUC Score\", roc_auc_score(actual, predicted_scores))\n",
    "        precision, recall, thresholds = precision_recall_curve(actual, predicted_scores)\n",
    "        mlflow.log_metric(\"PR AUC Score\", auc(recall, precision))\n",
    "        mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "In this stage of the model iteration we will invoke specific helper function (created in previous step) on the train and test data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate logger instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '[%(asctime)s] : %(levelname)s -> %(message)s', level = logging.DEBUG, filename = '../logs/pipeline_log.log')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_file(\"../configuration/pipeline_configuration.json\", logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_file(config[\"data_directory_path\"] + config[\"train_dataset_name\"], logger)\n",
    "test_data = read_file(config[\"data_directory_path\"] + config[\"test_dataset_name\"], logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_type_conversion(train_data, logger, config[\"type_conversion_mpping\"])\n",
    "test_data = data_type_conversion(test_data, logger, config[\"type_conversion_mpping\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = string_encoding(train_data, logger, config[\"string_columns\"])\n",
    "test_data = string_encoding(test_data, logger, config[\"string_columns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in config[\"imputation_method_mapping\"].keys():\n",
    "    train_data = missing_data_imputation(train_data, logger, column, method = config[\"imputation_method_mapping\"][column])\n",
    "    test_data = missing_data_imputation(test_data, logger, column, method = config[\"imputation_method_mapping\"][column]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Creating quartile features for Fare and Age to capture the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_eng_column in config[\"feature_engineering_columns\"]:\n",
    "    train_data = create_quantile_feature(train_data, logger, feature_eng_column, q = 4)\n",
    "    test_data = create_quantile_feature(test_data, logger, feature_eng_column, q = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_model_data = train_data.drop(columns = config[\"drop_columns\"])\n",
    "test_data_model_data = test_data.drop(columns = config[\"drop_columns\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation Split of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(train_data_model_data.columns).difference(set([config[\"target_column\"]])))\n",
    "target = config[\"target_column\"]\n",
    "X_train, X_val, Y_train, Y_val = split_train_validation(train_data_model_data, logger, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize / Scale the data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train[features], logger, config[\"data_directory_path\"] + config[\"scaler_file_name\"], fit_standard_scaler = True)\n",
    "X_val = normalize(X_val[features], logger, config[\"data_directory_path\"] + config[\"scaler_file_name\"], fit_standard_scaler = False)\n",
    "X_test = normalize(test_data_model_data[features], logger, config[\"data_directory_path\"] + config[\"scaler_file_name\"], fit_standard_scaler = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predictions_label = model.predict(X_val)\n",
    "predictions_score = model.predict_proba(X_val)[:, 1]\n",
    "log_metrics(Y_val, predictions_label, predictions_score, model, \"Logistic Regression Base\", 45, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
